# -*- coding: utf-8 -*-
"""YouTube3D

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kNX91X-h5VSXmMcY4QToUNmzh9FwvblT
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/princeton-vl/YouTube3D
# %cd YouTube3D

sh = """#!/bin/bash
fileid="1UM58PEwq3XXZv-cURk42RzEaTcuWF7cV"
filename="data_model.tar.gz"
curl -c ./cookie -s -L "https://drive.google.com/uc?export=download&id=${fileid}" > /dev/null
curl -Lb ./cookie "https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=${fileid}" -o ${filename}
"""

f = open("download_model.sh", "w")
f.write(sh)
f.close()

!sh download_model.sh

!tar -xzvf data_model.tar.gz

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/YouTube3D
import sys
sys.path.append("/content/YouTube3D/src")
print(sys.path)

#from RelativeLoss import RelativeLoss
#from RelativeDepthDataset import RelativeDepthDataset, relative_depth_collate_fn
#from DIWDataset import DIWDatasetVal
#from YoutubeDataset import YoutubeDatasetVal
#from ReDWebNet import ReDWebNet_resnet50
from HourglassNetwork import HourglassNetwork
import cv2
import torch, torch.nn as nn

model_file = "/content/YouTube3D/data_model/src/exp/Hourglass/models/best_model_iter_852000.bin"
model = HourglassNetwork().cpu()
_dict = torch.load(model_file,map_location=torch.device('cpu'))
model.load_state_dict(_dict)	
model.eval()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/YouTube3D

import torch
import numpy as np
import cv2

def vis_depth(depths, colors, i, target_res):
  out = depths[0,:]
  out = out - np.min(out[0, :, :])
  out = out / np.max(out[0, :, :]) * 255.0


  out = out.transpose(1, 2, 0)  

  out_color = colors[0,:]
  out_color = out_color.transpose(1, 2, 0)
  out_color[:,:,0] = (out_color[:,:,0] * 0.229 + 0.485 ) *255.0 
  out_color[:,:,1] = (out_color[:,:,1] * 0.224 + 0.456 ) *255.0 
  out_color[:,:,2] = (out_color[:,:,2] * 0.225 + 0.406 ) *255.0 
  img = np.zeros((out_color.shape[0],out_color.shape[1]*2,3), np.uint8)
  img[:,:out_color.shape[1], :] = out_color
  img[:,out_color.shape[1] : out_color.shape[1]*2, :] = out
  print(img.shape)
  img = cv2.resize(img, (2*target_res[1], target_res[0]))
  cv2.imwrite("%d_depth.jpg" % i, img)



from torch.autograd import Variable
#inputs=torch.rand((1,3,240,320),dtype=torch.float32)
#input_var = Variable(inputs.cpu())
im = cv2.imread("girl.png")
im = cv2.resize(im,(320,240))
im = np.asarray(im,dtype=np.float32) / 255
im = im - 0.5
imt = im.transpose(2, 0, 1)
inputs = torch.Tensor([imt])
print(inputs.shape)
input_var = Variable(inputs)
print(input_var.size(),input_var.type())
output_var = model(input_var.cpu())
print(output_var.size(),output_var.type())

target_res=[240,320]
vis_depth(output_var.data.cpu().numpy(), inputs.numpy(), 0, target_res)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/YouTube3D

def convert_to_onnx(net, output_name):
    input = torch.randn(1,3,240,320)
    input_names = ['data']
    output_names = ['output']
    net.eval()
    torch.onnx.export(net, input, output_name, verbose=True, input_names=input_names, output_names=output_names, opset_version=11)
convert_to_onnx(model, "YouTube3D.onnx")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/YouTube3D

import torch
import cv2
print(cv2.__version__)

net = cv2.dnn.readNet("YouTube3D.onnx")

!cp /content/YouTube3D/YouTube3D.onnx "/content/drive/My Drive/YouTube3D.onnx"

!cp "/content/drive/My Drive/cv2_cuda/cv2.cpython-36m-x86_64-linux-gnu.so" .