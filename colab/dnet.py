# -*- coding: utf-8 -*-
"""Dnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uDA36-sdqy4OvrRLBNHaVeq3NDedbGat
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/TJ-IPLab/DNet
# %cd DNet

!mkdir models
!unzip "/content/drive/My Drive/dnet_model.zip" 
!mv *.pth models

!python evaluate_depth.py --load_weights_folder models --eval_mono

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/DNet

import os
import cv2
import numpy as np

import torch
import torch.nn as nn

#from layers import disp_to_depth, ScaleRecovery
#from utils import readlines
#from options import MonodepthOptions
#import datasets
import networks


encoder = networks.ResnetEncoder(18, False)
encoder_dict = torch.load("models/encoder.pth")
encoder.eval()

 
depth_decoder = networks.DepthDecoder(encoder.num_ch_enc)

model_dict = encoder.state_dict()
encoder.load_state_dict({k: v for k, v in encoder_dict.items() if k in model_dict})
depth_decoder.load_state_dict(torch.load("models/depth.pth"))
depth_decoder.eval()

# 192,640

dummy_input = torch.randn(10, 3, 192, 640, device='cpu')
features = encoder(dummy_input)
outputs = depth_decoder(features)
print(type(features[-1]), len(features))
print(features[-1].stride())

for r in outputs: print(r.shape,r.stride())

class MonoDepth2(nn.Module):
    def __init__(self):
        super(MonoDepth2, self).__init__()
        self.encoder = encoder
        self.decoder = depth_decoder;
        self.outputs = {}
    def forward(self, input_features):
        features = self.encoder(input_features)
        #return features
        self.outputs = self.decoder(features)
        return self.outputs
mono = MonoDepth2()
torch.onnx.export(mono, dummy_input, "monodepth2.onnx", verbose=False)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/DNet

import cv2, numpy as np

net = cv2.dnn.readNet("monodepth2.onnx")
#inp = dummy_input.numpy() 
img = cv2.imread("kuche.png")
img = cv2.imread("bed.png")
#img = cv2.normalize(img,0,1,cv2.NORM_MINMAX)
inp = cv2.dnn.blobFromImage(img, 1.0/127, (640,192), (127,127,127), True, False)
net.setInput(inp)
names=["347","369","382","386"]
res = net.forward(names)
for r in res: print(r.shape)
net.dumpToFile("monodepth.dot")


def disp_to_depth(disp, min_depth=0.1, max_depth=100.0):
    """Convert network's sigmoid output into depth prediction
    The formula for this conversion is given in the 'additional considerations'
    section of the paper.
    """
    min_disp = 1 / max_depth
    max_disp = 1 / min_depth
    scaled_disp = min_disp + (max_disp - min_disp) * disp
    depth = 1 / scaled_disp
    return scaled_disp, depth


from google.colab.patches import cv2_imshow
im = res[0].reshape(192,640)
im = cv2.resize(im, (img.shape[1],img.shape[0]))
#im = res[1].reshape(96,320)
disp,depth = disp_to_depth(im)
cv2_imshow(img)
cv2_imshow((1.0/depth) * 32)
#cv2_imshow(disp * 32)
print(res[3])

!dot monodepth.dot -Tpng -omono.png
import cv2
im = cv2.imread("mono.png")
from google.colab.patches import cv2_imshow
cv2_imshow(im)

!cp monodepth2.onnx "/content/drive/My Drive"

!cp "/content/drive/My Drive/monodepth2.onnx" .

!cp "/content/drive/My Drive/cv2_cuda/cv2.cpython-36m-x86_64-linux-gnu.so" .