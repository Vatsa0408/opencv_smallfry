# -*- coding: utf-8 -*-
"""Copy of Pix2Pix

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X-erUr0UUaUyzpqTidZ6RDKiUcNQ3ocE

# Install
"""

!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix

import os
os.chdir('pytorch-CycleGAN-and-pix2pix/')

!pip install -r requirements.txt

"""# Datasets

Download one of the official datasets with:

-   `bash ./datasets/download_pix2pix_dataset.sh [cityscapes, night2day, edges2handbags, edges2shoes, facades, maps]`

Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets).
"""

!bash ./datasets/download_pix2pix_dataset.sh night2day

"""# Pretrained models

Download one of the official pretrained models with:

-   `bash ./scripts/download_pix2pix_model.sh [edges2shoes, sat2map, map2sat, facades_label2photo, and day2night]`

Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`
"""

# Pretrained models
!bash ./scripts/download_pix2pix_model.sh day2night

"""# Testing

-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`

Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.

> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:
> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.

> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).

> See a list of currently available models at ./scripts/download_pix2pix_model.sh
"""

!ls checkpoints/
!ls ./datasets/

#!python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name day2night_pretrained
!python test.py --dataroot ./datasets/night2day --name day2night_pretrained --model test --netG unet_256 --direction BtoA --dataset_mode single --norm batch

from google.colab.patches import cv2_imshow
import cv2, os
images = ['./datasets/night2day/test/14_1137_to_1114.jpg',
          './datasets/night2day/test/14_1137_to_1126.jpg',
          './datasets/night2day/test/14_1137_to_1151.jpg',
          './datasets/night2day/test/14_1137_to_1162.jpg',
          './datasets/night2day/test/14_1137_to_1167.jpg',
          './datasets/night2day/test/14_1137_to_1175.jpg',
          './datasets/night2day/test/14_1137_to_1183.jpg',
          './datasets/night2day/test/14_1137_to_1188.jpg',
          './datasets/night2day/test/14_1140_to_1114.jpg',
          './datasets/night2day/test/14_1140_to_1126.jpg']
for im in images:
  img = cv2.imread(im)
  cv2_imshow(img)

!ls -l

import os
from options.test_options import TestOptions
from data import create_dataset
from models import create_model


opt = TestOptions()#.parse()  # get test options
# hard-code some parameters for test
opt.verbose = False
opt.epoch='latest'
opt.load_iter = 0
opt.init_type = 'normal'
opt.init_gain = 0.02
opt.no_dropout = True
opt.input_nc = 3
opt.output_nc = 3
opt.ngf=64
opt.model_suffix = ""
opt.preprocess=False
opt.checkpoints_dir = ""
opt.gpu_ids=[]
opt.isTrain = False
opt.max_dataset_size = 20
opt.dataroot = "./datasets/night2day"
opt.name = "./checkpoints/day2night_pretrained"
opt.model = "test"
opt.netG = "unet_256"
opt.direction = "BtoA"
opt.dataset_mode = "single"
opt.norm = "batch"
opt.num_threads = 0   # test code only supports num_threads = 1
opt.batch_size = 1    # test code only supports batch_size = 1
opt.serial_batches = True  # disable data shuffling; comment this line if results on randomly chosen images are needed.
opt.no_flip = True    # no flip; comment this line if results on flipped images are needed.
opt.display_id = -1   # no visdom display; the test code saves the results to a HTML file.
#dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options
model = create_model(opt)      # create a model given opt.model and other options
model.setup(opt)               # regular setup: load and print networks; create schedulers

import torch
input_names = [ "input1" ]
output_names = [ "output1" ]
dummy_input = torch.randn(10, 3, 256, 256, device='cpu')

#torch.onnx.export(encoder, dummy_input, "encoder.onnx", verbose=False, input_names=input_names, output_names=output_names)
#torch.onnx.export(depth_decoder, dummy_features, "depth.onnx", verbose=False, input_names=input_names, output_names=output_names)
torch.onnx.export(model.netG, dummy_input, "day2night.onnx", verbose=True, input_names=input_names, output_names=output_names)

import cv2, numpy as np
net = cv2.dnn.readNet("day2night.onnx")
img = cv2.imread("house1.jpg")
#img.shape
mean = (128,128,128)
scale = 255
blob = cv2.dnn.blobFromImage(img, 1.0/scale, (256,256), mean, True)
net.setInput(blob)
res = net.forward()
print(res.shape)
res = res[0,:,:,:].transpose((1,2,0))
res *= scale
#res += mean
res = res.astype(np.uint8)
from google.colab.patches import cv2_imshow
cv2_imshow(img)
cv2_imshow(res)

from google.colab import files

uploaded = files.upload()

!ls -l *.onnx