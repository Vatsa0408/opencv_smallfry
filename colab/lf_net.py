# -*- coding: utf-8 -*-
"""lf-net

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MAcE-o6TNhwAq6RcOdM347dwrnF7GNm0
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/vcg-uvic/lf-net-release
# %cd lf-net-release
!wget http://webhome.cs.uvic.ca/~kyi/files/2018/lf-net/pretrained.tar.gz
!tar zxf pretrained.tar.gz
!ls -l

!ls -l release/models/outdoor
!mkdir export

#@title
import numpy as np
import tensorflow as tf
import cv2 as cv
from models import simple_desc

mdl = "./release/models/indoor/models-latest-42000"
height = 228
width = 304
channels = 3
batch_size = 1

# Build Networks
tf.reset_default_graph()


# Create a placeholder for the input image
input_node = tf.placeholder(tf.float32, shape=(None, height, width, channels))

# Construct the network
net = simple_desc.Model({'data': input_node,"desc_activ_fn":"Relu"}, False)
        
 
with tf.Session() as sess:
    # Restore session
    sess.graph.as_default()
    tf.import_graph_def(graph_def, name='')

    saver = tf.train.Saver()     
    saver.restore(sess, mdl)

    input_graph_def = sess.graph.as_graph_def()

    output_nodes_names=[ops['kpts'].op.name,ops['feats'].op.name,ops['scale_maps'].op.name,ops['kpts_scale'].op.name,ops['degree_maps'].op.name,ops['kpts_ori'].op.name]
    output_graph_def = graph_util.convert_variables_to_constants(
            sess, # The session
            input_graph_def, # input_graph_def is useful for retrieving the nodes 
            output_nodes_names  
    )
    output_graph_name="export/frozen.pb"
    with tf.gfile.GFile(output_graph_name, "wb") as f:
        f.write(output_graph_def.SerializeToString())
    tf.train.write_graph(output_graph_def, 'export/', 'frozen.pbtxt',as_text=True)

#!/usr/bin/env python
from __future__ import print_function
import os
import sys
import numpy as np
import tensorflow as tf
import importlib
import time
import cv2
from tqdm import tqdm
import pickle

from mydatasets import *

from det_tools import *
from eval_tools import draw_keypoints
from common.tf_train_utils import get_optimizer
from imageio import imread, imsave
from inference import *
from tensorflow.python.framework import graph_util
from tensorflow.python.tools import freeze_graph
from tensorflow.python.tools import optimize_for_inference_lib
from tensorflow.tools.graph_transforms import TransformGraph




MODEL_PATH = './models'
if MODEL_PATH not in sys.path:
    sys.path.append(MODEL_PATH)


def build_networks(config, photo, is_training):

    DET = importlib.import_module(config.detector)
    detector = DET.Model(config, is_training)
    if config.input_inst_norm:
        print('Apply instance norm on input photos')
        photos1 = instance_normalization(photo)

    if config.use_nms3d:
        heatmaps, det_endpoints = build_multi_scale_deep_detector_3DNMS(config, detector, photo, reuse=False)
    else:
        heatmaps, det_endpoints = build_multi_scale_deep_detector(config, detector, photo, reuse=False)

    # extract patches
    kpts = det_endpoints['kpts']
    batch_inds = det_endpoints['batch_inds']

    kp_patches = build_patch_extraction(config, det_endpoints, photo)

    # Descriptor
    DESC = importlib.import_module(config.descriptor)
    descriptor = DESC.Model(config, is_training)
    desc_feats, desc_endpoints = descriptor.build_model(kp_patches, reuse=False) # [B*K,D]
    print(detector)
    print(descriptor)
    # scale and orientation (extra)
    scale_maps = det_endpoints['scale_maps']
    ori_maps = det_endpoints['ori_maps'] # cos/sin
    degree_maps, _ = get_degree_maps(ori_maps) # degree (rgb psuedo color code)
    kpts_scale = det_endpoints['kpts_scale']
    kpts_ori = det_endpoints['kpts_ori']
    kpts_ori = tf.atan2(kpts_ori[:,1], kpts_ori[:,0]) # radian

    ops = {
        'photo': photo,
        'is_training': is_training,
        'kpts': kpts,
        'feats': desc_feats,
        # EXTRA
        'scale_maps': scale_maps,
        'kpts_scale': kpts_scale,
        'degree_maps': degree_maps,
        'kpts_ori': kpts_ori,
    }

    return ops

def main(config):
    print(config)
    # Build Networks
    tf.reset_default_graph()

    photo_ph = tf.placeholder(tf.float32, [1, None, None, 1]) # input grayscale image, normalized by 0~1
    is_training = tf.constant(False) # Always False in testing

    ops = build_networks(config, photo_ph, is_training)
    print(ops)
    tfconfig = tf.ConfigProto()
    tfconfig.gpu_options.allow_growth = True 
    sess = tf.Session(config=tfconfig)
    sess.run(tf.global_variables_initializer())

    # load model
    saver = tf.train.Saver()
    print('Load trained models...')

    if os.path.isdir(config.model):
        checkpoint = tf.train.latest_checkpoint(config.model)
        model_dir = config.model
    else:
        checkpoint = config.model
        model_dir = os.path.dirname(config.model)


    if checkpoint is not None:
        print('Checkpoint', os.path.basename(checkpoint))
        print("[{}] Resuming...".format(time.asctime()))
        saver.restore(sess, checkpoint)
    else:
        raise ValueError('Cannot load model from {}'.format(model_dir))    
    print('Done.')
   
    output_graph="export/normal.pb"
    print(tf.all_variables())
    #with tf.gfile.GFile(output_graph, "wb") as f:
    #    f.write(sess.graph.as_graph_def().SerializeToString())
    #tf.train.write_graph(sess.graph.as_graph_def(), 'export/', 'normaltxt.pbtxt',as_text=True)
  
    input_graph_def = sess.graph.as_graph_def()

    output_nodes_names=[ops['kpts'].op.name,ops['feats'].op.name,ops['scale_maps'].op.name,ops['kpts_scale'].op.name,ops['degree_maps'].op.name,ops['kpts_ori'].op.name]
    output_graph_def = graph_util.convert_variables_to_constants(
            sess, # The session
            input_graph_def, # input_graph_def is useful for retrieving the nodes 
            output_nodes_names  
    )
    # output_graph_def.save("export/frozen2.pb")
    output_graph_name="export/frozen.pb"
    #with tf.gfile.GFile(output_graph_name, "wb") as f:
    #    f.write(output_graph_def.SerializeToString())
    #tf.train.write_graph(output_graph_def, 'export/', 'frozen.pbtxt',as_text=True)



    inp_node = ['Placeholder']
    optimize_graph_def = optimize_for_inference_lib.optimize_for_inference(output_graph_def, inp_node, output_nodes_names,
                                                                tf.float32.as_datatype_enum)
    optimize_graph_def = TransformGraph(optimize_graph_def, inp_node, output_nodes_names, ["sort_by_execution_order"])

    removeUnusedNodesAndAttrs(to_remove, optimize_graph_def)

    output_graph_name="export/optimize.pb"
    with tf.gfile.GFile(output_graph_name, "wb") as f:
        f.write(optimize_graph_def.SerializeToString())
    tf.train.write_graph(optimize_graph_def, "export/", 'optimize.pbtxt', as_text=True)

def to_remove(name, op):
    #if name in ["Atan2_1",'strided_slice_1', 'strided_slice_2']: return True
    return False
    #(not name in nodesToKeep) and \
    #           (op == 'Const' or (not op in keepOps) or name.startswith(prefixesToRemove))

def removeUnusedNodesAndAttrs(to_remove, graph_def):
    unusedAttrs = ['T', 'Tshape', 'N', 'Tidx', 'Tdim', 'use_cudnn_on_gpu',
                   'Index', 'Tperm', 'is_training', 'Tpaddings']

    removedNodes = []

    for i in reversed(range(len(graph_def.node))):
        op = graph_def.node[i].op
        name = graph_def.node[i].name

        if to_remove(name, op):
            if op != 'Const':
                removedNodes.append(name)
                print("remove op",name, graph_def.node[i].input)

            del graph_def.node[i]
        else:
            print("keep op",name, graph_def.node[i].input)

            for attr in unusedAttrs:
                if attr in graph_def.node[i].attr:
                    del graph_def.node[i].attr[attr]

    # Remove references to removed nodes except Const nodes.
    for node in graph_def.node:
        for i in reversed(range(len(node.input))):
            if node.input[i] in removedNodes:
                print("remove inp",node.input[i])
                del node.input[i]




if __name__ == '__main__':

    from common.argparse_utils import *
    parser = get_parser()

    general_arg = add_argument_group('General', parser)
    general_arg.add_argument('--num_threads', type=int, default=8,
                            help='the number of threads (for dataset)')

    io_arg = add_argument_group('In/Out', parser)
    io_arg.add_argument('--in_dir', type=str, default='./samples',
                            help='input image directory')
    # io_arg.add_argument('--in_dir', type=str, default='./release/outdoor_examples/images/sacre_coeur/dense/images',
    #                         help='input image directory')
    io_arg.add_argument('--out_dir', type=str, default='./dump_feats',
                            help='where to save keypoints')
    io_arg.add_argument('--full_output', type=str2bool, default=True,
                            help='dump keypoint image')

    model_arg = add_argument_group('Model', parser)
    model_arg.add_argument('--model', type=str, default='release/models/outdoor/',
                            help='model file or directory')
    model_arg.add_argument('--top_k', type=int, default=500,
                            help='number of keypoints')
    model_arg.add_argument('--max_longer_edge', type=int, default=640,
                            help='resize image (do nothing if max_longer_edge <= 0)')

    tmp_config, unparsed = get_config(parser)

    # if len(unparsed) > 0:
    #    raise ValueError('Miss finding argument: unparsed={}\n'.format(unparsed))

    # restore other hyperparams to build model
    if os.path.isdir(tmp_config.model):
        config_path = os.path.join(tmp_config.model, 'config.pkl')
    else:
        config_path = os.path.join(os.path.dirname(tmp_config.model), 'config.pkl')
    try:
        with open(config_path, 'rb') as f:
            config = pickle.load(f)
            print(config)
    except:
        raise ValueError('Fail to open {}'.format(config_path))

    for attr, dst_val in sorted(vars(tmp_config).items()):
        if hasattr(config, attr):
            src_val = getattr(config, attr)
            if src_val != dst_val:
                setattr(config, attr, dst_val)
        else:
            setattr(config, attr, dst_val)

    main(config)

!ls -l export
tf.__version__

import cv2
cv2.dnn.readNet("export/optimize.pb")