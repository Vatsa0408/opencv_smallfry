# -*- coding: utf-8 -*-
"""VNL_Monocular_Depth_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NIzqO9ATJCcOVDR8xTP4t8yRy_bVA0de
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/YvanYin/VNL_Monocular_Depth_Prediction
# %cd VNL_Monocular_Depth_Prediction

!wget https://cloudstor.aarnet.edu.au/plus/s/7kdsKYchLdTi53p/download -O ResNext101_32x4d_NYU.pth

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/VNL_Monocular_Depth_Prediction

import os
import cv2
import torch
import numpy as np
from lib.models.metric_depth_model import MetricDepthModel
from lib.core.config import cfg, merge_cfg_from_file
import torchvision.transforms as transforms
#from lib.models.image_transfer import bins_to_depth

def load_ckpt(fn, model, optimizer=None, scheduler=None, val_err=[]):
    # checkpoint = torch.load(fn, map_location=lambda storage, loc: storage, pickle_module=dill)
    checkpoint = torch.load(fn, map_location=lambda storage, loc: storage)
    model.load_state_dict(checkpoint['model_state_dict'])
 
def bins_to_depth(depth_bin):
    """
    Transfer n-channel discrate depth bins to 1-channel conitnuous depth
    :param depth_bin: n-channel output of the network, [b, c, h, w]
    :return: 1-channel depth, [b, 1, h, w]
    """
    depth = depth_bin.permute(0, 2, 3, 1) #[b, h, w, c]
#    depth = depth_bin * cfg.DATASET.DEPTH_BIN_BORDER
    depth = torch.sum(depth, dim=3, dtype=torch.float32, keepdim=True)
    depth = 10 ** depth
    depth = depth.permute(0, 3, 1, 2)  # [b, 1, h, w]
    return depth



def scale_torch(img, scale):
    """
    Scale the image and output it in torch.tensor.
    :param img: input image. [C, H, W]
    :param scale: the scale factor. float
    :return: img. [C, H, W]
    """
    img = np.transpose(img, (2, 0, 1))
    img = img[::-1, :, :]
    img = img.astype(np.float32)
    img /= scale
    img = torch.from_numpy(img.copy())
    img = transforms.Normalize(cfg.DATASET.RGB_PIXEL_MEANS, cfg.DATASET.RGB_PIXEL_VARS)(img)
    return img

model = MetricDepthModel()
model.eval()
load_ckpt("/content/VNL_Monocular_Depth_Prediction/ResNext101_32x4d_NYU.pth", model)
with torch.no_grad():
    img = cv2.imread("/content/VNL_Monocular_Depth_Prediction/test_any_imgs_examples/107_r.png")
    img_resize = cv2.resize(img, (int(img.shape[1]), int(img.shape[0])), interpolation=cv2.INTER_LINEAR)
    img_torch = scale_torch(img_resize, 255)
    img_torch = img_torch[None, :, :, :].cpu()

    _, pred_depth_softmax= model.depth_model(img_torch)
    pred_depth = bins_to_depth(pred_depth_softmax)
    pred_depth = pred_depth.cpu().numpy().squeeze()
    pred_depth_scale = (pred_depth / pred_depth.max() * 60000).astype(np.uint16)  # scale 60000 for visualization

    cv2.imwrite("depth.png", pred_depth_scale)



def convert_to_onnx(net, output_name):
    input = torch.randn(1,3,240,320)
    input_names = ['data']
    output_names = ['output']
    net.eval()
    torch.onnx.export(net, input, output_name, verbose=True, input_names=input_names, output_names=output_names, opset_version=11)
convert_to_onnx(model, "VNL.onnx")

import torch
torch.__version__