# -*- coding: utf-8 -*-
"""monodepth2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13_HWItWk1h5Jn7UJDMMDAZ0kaGidpWpf
"""

!pip install opencv-python==4.1.1.26

!git clone https://github.com/nianticlabs/monodepth2

# Commented out IPython magic to ensure Python compatibility.
# %cd monodepth2
!ls -l

#!wget https://drive.google.com/uc?export=download&id=1Bk9gMrzuF_QrDRv11ILrqv3xHvqxZR2a -O mono.tf
#!curl https://drive.google.com/uc?export=download&id=1Bk9gMrzuF_QrDRv11ILrqv3xHvqxZR2a
#!wget https://drive.google.com/open?id=1Bk9gMrzuF_QrDRv11ILrqv3xHvqxZR2a -O mono.tf
#!wget https://drive.google.com/uc?export=download&confirm=dGIM&id=1Bk9gMrzuF_QrDRv11ILrqv3xHvqxZR2a -O mono.tf
#!wget https://accounts.google.com/ServiceLogin?service=wise&passive=1209600&continue=https://docs.google.com/nonceSigner?nonce%3Dbldnuuh7ucvn8%26continue%3Dhttps://doc-0o-2s-docs.googleusercontent.com/docs/securesc/5nt72vshr5ipuriuhbe5ge57k9sb6p4i/h6n2gaajvijuc542sj54j3b7kuasjgkc/1573732800000/10943321048839037113/06093239409352276762/1Bk9gMrzuF_QrDRv11ILrqv3xHvqxZR2a?e%253Ddownload%26hash%3Dupceb9al3u0o2o6o19kbuj0oilaa9eb6&followup=https://docs.google.com/nonceSigner?nonce%3Dbldnuuh7ucvn8%26continue%3Dhttps://doc-0o-2s-docs.googleusercontent.com/docs/securesc/5nt72vshr5ipuriuhbe5ge57k9sb6p4i/h6n2gaajvijuc542sj54j3b7kuasjgkc/1573732800000/10943321048839037113/06093239409352276762/1Bk9gMrzuF_QrDRv11ILrqv3xHvqxZR2a?e%253Ddownload%26hash%3Dupceb9al3u0o2o6o19kbuj0oilaa9eb6
!wget https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip

!unzip mono_no_pt_640x192.zip
!ls -l

import torch
import torch.nn as nn
#from torchvision import transforms, datasets

import networks
#from layers import disp_to_depth
#from utils import download_model_if_doesnt_exist

if torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")
print(device)

# LOADING PRETRAINED MODEL
print("   Loading pretrained encoder")
encoder = networks.ResnetEncoder(18, False)
loaded_dict_enc = torch.load("encoder.pth", map_location=device)

# extract the height and width of image that this model was trained with
feed_height = loaded_dict_enc['height']
feed_width = loaded_dict_enc['width']
filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}
encoder.load_state_dict(filtered_dict_enc)
encoder.to(device)
encoder.eval()


print("   Loading pretrained decoder")
depth_decoder = networks.DepthDecoder(
    num_ch_enc=encoder.num_ch_enc, scales=range(4))

loaded_dict = torch.load("depth.pth", map_location=device)
depth_decoder.load_state_dict(loaded_dict)

depth_decoder.to(device)
depth_decoder.eval()

class MonoDepth2(nn.Module):
    def __init__(self):
        super(MonoDepth2, self).__init__()
        self.encoder = encoder
        self.decoder = depth_decoder;
        self.outputs = {}
    def forward(self, input_features):
        features = self.encoder(input_features)
        self.outputs = self.decoder(features)
        return self.outputs

mono = MonoDepth2()

dummy_input = torch.randn(10, 3, 192, 640, device='cpu')
features = encoder(dummy_input)
outputs = depth_decoder(features)
print(type(features[-1]), len(features))
print(features[-1].stride())

dummy_features = [
                  torch.randn(10, 64, 96, 320),
                  torch.randn(10, 64, 48, 160),
                  torch.randn(10, 128, 24, 80),
                  torch.randn(10, 256, 12, 40),
                  torch.randn(10, 512, 6, 20) ]
out2 = depth_decoder(dummy_features)

input_names = [ "input1" ]
output_names = [ "output1" ]
#torch.onnx.export(encoder, dummy_input, "encoder.onnx", verbose=False, input_names=input_names, output_names=output_names)
#torch.onnx.export(depth_decoder, dummy_features, "depth.onnx", verbose=False, input_names=input_names, output_names=output_names)
torch.onnx.export(mono, dummy_input, "monodepth2.onnx", verbose=False, input_names=input_names, output_names=output_names)

!ls -l
import cv2, numpy as np

enc = cv2.dnn.readNet("encoder.onnx")
dec = cv2.dnn.readNet("depth.onnx")